########################
# Common config
data.directory=data
bin.directory=bin
kvstore.entrydb.path=work/kvs/idb
kvstore.delay.milliseconds=200
temporary.file.directory.suffix=[UNIQUE]
temporary.file.directory=temp/${temporary.file.directory.suffix}
delete.temporary.directory.on.completion=false

# JMS Broker Configuration
jms.broker.host.name=servername.ebi.ac.uk
jms.broker.port.number=5445

# Embedded JMS Broker Configuration
jms.broker.temp.directory=activemq-data/localhost/tmp_storage

# JDBC Configuration
i5.database.driverClassName=org.h2.Driver
i5.database.driverJar=
i5.database.connection.url=jdbc:h2:mem:interpro;LOCK_TIMEOUT=10000
i5.database.username=sa
i5.database.password=

# Clean database installation (only used by embedded master)
i5.h2.database.original.location=work/template/interpro.zip

# Hibernate Configuration
hibernate.dialect=org.hibernate.dialect.HSQLDialect
hibernate.show_sql=false
hibernate.format_sql=true
hibernate.default_schema=PUBLIC
hibernate.use_sql_comments=false
hibernate.hbm2ddl_auto=create
# Determines the number of updates (inserts, updates and deletes) that are sent to the database at one time for execution
# Docs recommend a value between 5 and 30.
hibernate.jdbc.batch_size=30
# Sets the statement's fetch size within the JDBC driver - the number of rows fetched when there is a multiple row result on select statements (default is 0, param disabled)
hibernate.jdbc.fetch_size=4000
# Sets a maximum depth for the outer join fetch tree for single-ended associations. A single-ended assocation is a one-to-one or many-to-one assocation.
# Requires a value between 0 - 3 (0 disables default outer join fetching).
hibernate.max_fetch_depth=3
# Hibernate c3p0 connection pool
# Minimum number of JDBC connections in the pool. Hibernate default: 1
hibernate.c3p0.min_size=5
# Maximum number of JDBC connections in the pool. Hibernate default: 100
hibernate.c3p0.max_size=150
# The number of new JDBC connections in the pool to acquire at a time. Hibernate default: 1
hibernate.c3p0.acquire_increment=5
# Idle time in seconds before a connection is automatically validated. Hibernate default: 0
hibernate.c3p0.idle_test_period=3000
# Number of prepared statements will be cached. Increase performance. Hibernate default: 0 , caching is disabled
hibernate.c3p0.max_statements=300
# When an idle connection is removed from the pool (in seconds). Hibernate default: 0, never expire
hibernate.c3p0.timeout=1800
## to get rid of the error:  WARN  org.hibernate.boot.internal.InFlightMetadataCollectorImpl - HHH000069: Duplicate generator name SIG_LIB_IDGEN
hibernate.id.new_generator_mappings=true

## freemarker
freemarker.path=work/freemarker

########################
# Master config

unix.username=username

########################
# Worker config
jvm.maximum.idle.time.seconds=180
jvm.maximum.life.seconds=172400

# Configure the version of perl to use when running member databases perl binaries
perl.command=perl
python3.command=python3

# HMMER 2
binary.hmmer2.hmmsearch.path=${bin.directory}/hmmer/hmmer2/2.3.2/hmmsearch
binary.hmmer2.hmmpfam.path=${bin.directory}/hmmer/hmmer2/2.3.2/hmmpfam

# HMMER 3
binary.hmmer3.path=${bin.directory}/hmmer/hmmer3/3.3
binary.hmmer3.hmmscan.path=${bin.directory}/hmmer/hmmer3/3.3/hmmscan
binary.hmmer3.hmmsearch.path=${bin.directory}/hmmer/hmmer3/3.3/hmmsearch

# Misc
binary.getorf.path=${bin.directory}/nucleotide/getorf
binary.esltranslate.path=${bin.directory}/nucleotide/esl-translate

# AntiFam
antifam.signature.library.release=8.0
antifam.hmm.path=${data.directory}/antifam/8.0/AntiFam.hmm
hmmer3.hmmsearch.cpu.switch.antifam=--cpu 1
hmmer3.hmmsearch.switches.antifam=--cut_ga

#CDD
cdd.signature.library.release=3.21
cdd.signature.list.path=${data.directory}/cdd/3.21/data/cddid.tbl
cdd.library.path=${data.directory}/cdd/3.21/db/Cdd_NCBI
cdd.data.path=${data.directory}/cdd/3.21/data
binary.rpsblast.path=${bin.directory}/cdd/rpsblast
rpsblast.switches.cdd=-evalue 0.01 -seg no -outfmt 11
binary.rpsbproc.path=${bin.directory}/cdd/rpsbproc
rpsbproc.switches.cdd=-m std

# Coils
coils.signature.library.release=2.2.1
binary.coils.path=${bin.directory}/ncoils/2.2.1/ncoils
coils.binary.switches=-c

#GENE3D
gene3d.signature.library.release=4.3.0
cath.resolve.hits.path=${bin.directory}/gene3d/4.3.0/cath-resolve-hits
assign.cath.superfamilies.path=${bin.directory}/gene3d/4.3.0/assign_cath_superfamilies.py
assign.cath.superfamilies.switches.gene3d=--min-dc-hmm-coverage=80 --worst-permissible-bitscore 25 --output-hmmer-aln
gene3d.hmm.path=${data.directory}/gene3d/4.3.0/gene3d_main.hmm
gene3d.model2sf_map.path=${data.directory}/gene3d/4.3.0/model_to_family_map.tsv
gene3d.discontinuous_regs.path=${data.directory}/gene3d/4.3.0/discontinuous_regs.pkl.py3
gene3d.hmmsearch.force=true
hmmer3.hmmsearch.cpu.switch.gene3d=--cpu 1
hmmer3.hmmsearch.switches.gene3d=-Z 65245 -E 0.001
gene3d.evalue.cutoff=0.001
cath.resolve.hits.switches.gene3d=--min-dc-hmm-coverage=80 --worst-permissible-bitscore 25 --output-hmmer-aln

# FunFam
funfam.signature.library.release=4.3.0
binary.funfam.path=${bin.directory}/funfam/4.3.0/search.py
funfam.hmm.path=${data.directory}/funfam/4.3.0/models/
hmmer3.hmmsearch.cpu.switch.funfam=--cpu 1
cath.resolve.hits.switches.funfam=--min-dc-hmm-coverage=80 --worst-permissible-bitscore 25 --output-hmmer-aln
hmmer3.hmmsearch.switches.funfam=-Z 65245 --cut_tc

# HAMAP
binary.hamap.pfsearch.wrapperpath=${bin.directory}/prosite/pfsearch_wrapper.py
hamap.signature.library.release=2023_05
hamap.profile.models.path=${data.directory}/hamap/2023_05/hamap.prf
hamap.profile.models.dir=${data.directory}/hamap/2023_05/profiles
hamap.hmm.path=${data.directory}/hamap/2023_05/hamap.hmm.lib
hmmer3.hmmsearch.cpu.switch.hmmfilter=--cpu 1
hmmer3.hmmsearch.switches.hmmfilter=-E 100 --domE 100 --incE 100 --incdomE 100
pfsearchv3.hamap.binary.switches=-f -o 7

# MobiDB
mobidb.signature.library.release=4.0
binary.mobidb.path=${bin.directory}/mobidb/idrpred-cli.py
mobidb.binary.cpu.switches=--threads 1

# NCBIfam
ncbifam.signature.library.release=17.0
ncbifam.hmm.path=${data.directory}/ncbifam/17.0/ncbifam.hmm
hmmer3.hmmsearch.switches.ncbifam=-Z 61295632 --cut_tc
hmmer3.hmmsearch.cpu.switch.ncbifam=--cpu 1

# PANTHER
panther.signature.library.release=19.0
binary.epang.path=${bin.directory}/panther/epa-ng
binary.treegrafter.path=${bin.directory}/panther/treegrafter.py
panther.binary.control.factor=1
panther.models.dir=${data.directory}/panther/19.0/
panther.hmm.path=${data.directory}/panther/19.0/famhmm/binHmm
panther.hmmsearch.force=true
hmmer3.hmmsearch.cpu.switch.panther=--cpu 1
hmmer3.hmmsearch.switches.panther=-Z 65000000 -E 0.001 --domE 0.00000001 --incdomE 0.00000001 --notextw
panther.binary.switches=-e 0.00000001 --keep

# PFam
pfam-a.signature.library.release=37.3
pfam-a.hmm.path=${data.directory}/pfam/37.3/pfam_a.hmm
pfam-a.seed.path=${data.directory}/pfam/37.3/pfam_a.seed
pfam-clans.path=${data.directory}/pfam/37.3/pfam_clans
pfam-a.dat.path=${data.directory}/pfam/37.3/pfam_a.dat
hmmer3.hmmsearch.cpu.switch.pfama=--cpu 1
hmmer3.hmmsearch.switches.pfama=-Z 61295632 --cut_ga
pfam.min.length=8

# PHOBIUS
# Note: Phobius binary not distributed with InterProScan 5, please install separately
phobius.signature.library.release=1.01
binary.phobius.pl.path=${bin.directory}/phobius/1.01/phobius.pl

# PIRSF
pirsf.signature.library.release=3.10
binary.pirsf.pl.path=${bin.directory}/pirsf/3.10/pirsf.pl
pirsf.sfhmm.path=${data.directory}/pirsf/3.10/sf_hmm_all
pirsf.dat.path=${data.directory}/pirsf/3.10/pirsf.dat
pirsf.hmmsearch.force=false
hmmer3.hmmsearch.cpu.switch.pirsf=--cpu 1
pirsf.pl.binary.cpu.switch=-cpu 1
pirsf.pl.binary.switches=--outfmt i5
hmmer3.hmmsearch.switches.pirsf=-E 0.01 --acc

# PIRSR
pirsr.signature.library.release=2023_05
pirsr.binary.path=${bin.directory}/pirsr/0.1/pirsr.py
pirsr.srhmm.path=${data.directory}/pirsr/2023_05/sr_hmm_all
pirsr.data.path=${data.directory}/pirsr/2023_05/
pirsr.rules.path=${data.directory}/pirsr/2023_05/sr_uru.json
hmmer3.hmmsearch.cpu.switch.pirsr=--cpu 1
pirsr.hmmsearch.force=true
hmmer3.hmmsearch.switches.pirsr=-E 0.01 --acc

# PRINTS
prints.signature.library.release=42.0
binary.fingerprintscan.path=${bin.directory}/prints/fingerPRINTScan
prints.kdat.path=${data.directory}/prints/42.0/prints.kdat
prints.pval.path=${data.directory}/prints/42.0/prints.pval
prints.hierarchy.path=${data.directory}/prints/42.0/FingerPRINTShierarchy.db
fingerprintscan.binary.switches=-e 0.0001 -d 10 -E 257043 84355444 -fj -o 15

# PROSITE
# Note: Correct PROSITE binary distribution for your platform can be downloaded: ftp://ftp.expasy.org/databases/prosite/ps_scan/
prosite.patterns.signature.library.release=2025_01
prosite.profiles.signature.library.release=2025_01
binary.prosite.psscan.pl.path=${bin.directory}/prosite/ps_scan.pl
binary.prosite.pfscanv3.path=${bin.directory}/prosite/pfscanV3
binary.prosite.pfsearchv3.path=${bin.directory}/prosite/pfsearchV3
binary.prosite.runprosite.path=${bin.directory}/prosite/runprosite.py
prosite.patterns.models.path=${data.directory}/prosite/2025_01/prosite_patterns.dat
prosite.profiles.models.path=${data.directory}/prosite/2025_01/prosite_profiles.dat
prosite.profiles.models.dir=${data.directory}/prosite/2025_01/prosite_profiles
prosite.evaluator.models.path=${data.directory}/prosite/2025_01/evaluator.dat
prosite.profiles.skip.flagged.profiles=${data.directory}/prosite/2025_01/skip_flagged_profiles.txt
psscan.prosite.profiles.usepfsearch=true
pfsearchv3.binary.switches.prosite.profiles=-f -o 7
pfsearchv3.cpu.switch.prosite.profiles=-t 4
psscan.prosite.patterns.binary.switches=-r -s -o ipro

# SFLD
sfld.signature.library.release=4
sfld.postprocess.command=${bin.directory}/sfld/sfld_postprocess
sfld.hmm.path=${data.directory}/sfld/4/sfld.hmm
sfld.sites.annotation.file.path=${data.directory}/sfld/4/sfld_sites.annot
sfld.hierarchy.file.path=${data.directory}/sfld/4/sfld_hierarchy_flat.txt
hmmer3.hmmsearch.cpu.switch.sfld=--cpu 1
sfld.hmmsearch.force=true
hmmer3.hmmsearch.switches.sfld=-Z 378 --acc --cut_ga

# SMART
smart.signature.library.release=9.0
smart.hmm.path=${data.directory}/smart/9.0/smart.HMMs
smart.hmm.bin.path=${data.directory}/smart/9.0/smart.HMMs.bin
smart.overlapping.path=
smart.threshold.path=
hmmer2.hmmpfam.cpu.switch.smart=--cpu 1
hmmer2.hmmpfam.switches.smart=--acc -A 0

# SUPERFAMILY
superfamily.signature.library.release=1.75
binary.superfamily.ass3.pl.path=${bin.directory}/superfamily/1.75/ass3_single_threaded.pl
superfamily.hmm.path=${data.directory}/superfamily/1.75/hmmlib_1.75
superfamily.self.hits.path=${data.directory}/superfamily/1.75/self_hits.tab
superfamily.cla.path=${data.directory}/superfamily/1.75/dir.cla.scop.txt_1.75
superfamily.model.tab.path=${data.directory}/superfamily/1.75/model.tab
superfamily.pdbj95d.path=${data.directory}/superfamily/1.75/pdbj95d
hmmer3.hmmsearch.cpu.switch.superfamily=--cpu 1
hmmer3.hmmsearch.switches.superfamily=-E 10 -Z 15438

# SignalP
# Note: SignalP binary not distributed with InterProScan 5, please install separately
signalp_euk.signature.library.release=4.1
signalp_gram_positive.signature.library.release=4.1
signalp_gram_negative.signature.library.release=4.1
binary.signalp.path=${bin.directory}/signalp/4.1/signalp
signalp.perl.library.dir=${bin.directory}/signalp/4.1/lib
signalp.euk.binary.switches=-t euk -f summary -c 70
signalp.gramnegative.binary.switches=-t gram- -f summary -c 70
signalp.grampositive.binary.switches=-t gram+ -f summary -c 70

# TMHMM
# Note: TMHMM binary not distributed with InterProScan 5, please install separately
tmhmm.signature.library.release=2.0c
binary.tmhmm.path=${bin.directory}/tmhmm/2.0c/decodeanhmm.Linux_x86_64
tmhmm.model.path=${bin.directory}/tmhmm/2.0c/TMHMM2.0.model
tmhmm.binary.switches=-N 1 -PrintNumbers

# Max number of proteins per analysis batch
# These values control the maximum number of proteins put through
# an analysis in one go - different algorithms have different optimum values.
# Note that if you suffer from out of memory errors, reducing these values
# will almost certainly help, but may reduce the speed of analysis.
analysis.max.sequence.count.ANTIFAM=5000
analysis.max.sequence.count.CDD=1000
analysis.max.sequence.count.COILS=5000
analysis.max.sequence.count.FUNFAM=4000
analysis.max.sequence.count.GENE3D=4000
analysis.max.sequence.count.HAMAP=32000
analysis.max.sequence.count.MOBIDB_LITE=5000
analysis.max.sequence.count.NCBIFAM=5000
analysis.max.sequence.count.PANTHER=500
analysis.max.sequence.count.PFAM=5000
analysis.max.sequence.count.PHOBIUS=5000
analysis.max.sequence.count.PIRSF=4000
analysis.max.sequence.count.PIRSR=4000
analysis.max.sequence.count.PRINTS=500
analysis.max.sequence.count.PRODOM=5000
analysis.max.sequence.count.PROSITE_PATTERNS=5000
analysis.max.sequence.count.PROSITE_PROFILES=2000
analysis.max.sequence.count.SFLD=16000
# SignalP 4.1 binary only allows a maximum of 10,000 sequences
analysis.max.sequence.count.SIGNALP=4000
analysis.max.sequence.count.SSF=3000
analysis.max.sequence.count.SMART=500
analysis.max.sequence.count.TMHMM=5000

# When loading a set of models / signatures into the database, include the abstract in the database.
signature.store.abstracts=false

# Command to start a new worker (new jvm)
worker.command=java -Xms2028M -Xmx9216M -jar interproscan-5.jar
# This may be identical to the worker.command argument above, however you may choose to select
# a machine with a much larger available memory, for use when a StepExecution fails.
worker.high.memory.command=java -Xms2028M -Xmx9216M -jar interproscan-5.jar

#log dir
log.dir=logs

# Grid name
grid.name=lsf

# Project name for this run
user.digest=i5GridRun

# Grid jobs limit : number of jobs you are allowed to run on the cluster
grid.jobs.limit=3000

# Time between each bjobs/qstat command to check the status of jobs on the cluster
grid.check.interval.seconds=120

# Grid commands
grid.master.submit.command=bsub
grid.master.submit.high.memory.command=bsub -M 8192
grid.worker.submit.command=bsub
grid.worker.submit.high.memory.command=bsub -M 8192
grid.command.heredoc.open=cat << EOS |
grid.command.heredoc.close=EOS

#sge qsub commands
#grid.sge.worker.command=qsub -N i5t2worker
#grid.sge.worker.high.memory.command=qsub -N i5t2hmworker
#grid.sge.master.command=qsub -N i5t1worker
#grid.sge.master.high.memory.command=qsub -N i5t1hmworker

master.can.run.binaries=false

# Deal with unknown step states
recover.unknown.step.state=false

number.of.embedded.workers=1
maxnumber.of.embedded.workers=4
steps.to.consumer.ratio=6
master.steps.to.consumer.ratio=10
print.worker.summary=false

#number of embedded workers  for the master  (can run non-binary step)
thinmaster.number.of.embedded.workers=1
thinmaster.maxnumber.of.embedded.workers=1

# Max number of connections to the master
master.maxconsumers=48

# Number of connections to the worker
worker.maxconsumers=32

max.tier.depth=1

#number of embedded workers for the workers
worker.number.of.embedded.workers=4
worker.maxnumber.of.embedded.workers=4

#throttled net
grid.throttle=true
worker.maxunfinished.jobs=16

# Spare worker creation
spare.worker.start.delay.seconds=120
spare.worker.repeat.interval.seconds=120

# If multiple hosts are sharing the same file system, a delay may be required to
# avoid stale NFS handles
nfs.delay.milliseconds=5000

# Precalculated match lookup service
precalculated.match.lookup.service.url=https://www.ebi.ac.uk/interpro/match-lookup
precalculated.match.protein.lookup.batch.size=200
precalculated.match.protein.insert.batch.size=500
precalculated.match.protein.insert.batch.size.nolookup=8000
precalculated.match.lookup.service.proxy.host=
precalculated.match.lookup.service.proxy.port=3128
exclude.sites.from.output=false

# EBI Specific settings
ebi.uniparc.protein.load.size=1
ebi.uniparc.protein.load.start.delay.seconds=1
ebi.uniparc.protein.load.repeat.interval.seconds=1800000

# getorf configuration for nucleic acid sequences
# Approximate times getorf takes to find sequences of open reading frames (ORFs)
# in n nucleotide sequences:
#     600000 sequences: 10 minutes
#    3600000 sequences: 1 hour
#    7200000 sequences: 2 hours
#   43200000 sequences: 12 hours
getorf.minsize=75

# Set InterProScan to only process the N longest ORFs for each nucleotide sequence
binary.getorf.parser.path=${bin.directory}/nucleotide/parseOrfs.py
binary.getorf.parser.filtersize=8
getorf.parser.binary.switches=

#log4j file
log4j.log.file=i5.log4j.out.txt
log4j.jms.level=warn

#verbose
verbose.log=false
verbose.log.level=-99

# Output format
# TRUE by default, which means all generated graphical output documents (only SVG at the moment) will be archived (using the Linux command tar).
# This simple switch allows you to switch the archive mode off (simply set it to FALSE).
archiveSVGOutput=true

#control threads during the final prepare output step
max.concurrent.threads.for.prepare.output.step=1

#gc
periodic.gc.call=true

#time delay for resources monitor in seconds
resource.monitor.time.delay=5

#experimental depending on the db - check h2 mvcc
max.serial.group.executions=1

# production-specific settings
production.max.serial.group.executions=1

##singleseq mode
binary.run.delay=30
check.fork.progress=true

# more jms properties
consumer.prefetch.limit=2
