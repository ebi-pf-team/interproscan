##############################################################################
## This configuration/property file is suitable for INTERNAL USE ONLY in PRODUCTION mode. #
## NOT suitable for public use.
##
## Setup steps (you have to do before running I5 in master mode):
## Run command: ./interproscan.sh -mode master
## 1) Set Oracle connection details/Replace question marks (i5.database.connection.url ect.)
## 2) Set the database schema name/Replace question mark (property 'hibernate.default_schema')
## 3) Set appropriate paths to the member database binaries/models (e.g. )
##############################################################################
#
## The text [UNIQUE], if present, will be replaced by a value unique to your running instance
#
## Temporary files used by the analyses will be placed in directories here:
#temporary.file.directory.suffix=[UNIQUE]
#temporary.file.directory=/tmp/${temporary.file.directory.suffix}
#
#i5.database.driverClassName=uk.ac.ebi.interpro.scan.persistence.oracle.OracleCustomDriver
#i5.database.driverJar=
#i5.database.connection.url=?
#i5.database.username=?
#i5.database.password=?
#
## Hibernate Configuration
##hibernate.hbm2ddl_auto=create
#hibernate.dialect=uk.ac.ebi.interpro.scan.persistence.oracle.OracleCustomDialect
#hibernate.show_sql=false
#hibernate.format_sql=true
#hibernate.default_schema=?
#hibernate.use_sql_comments=false
#hibernate.jdbc.batch_size=30
#
## Configure the version of perl to use when running member databases perl binaries
#perl.command=perl
#
## Binary file locations
#binary.hmmer3.path=/ebi/production/interpro/binaries/64_bit_Linux/HMMER3.1b1
#binary.hmmer3.hmmscan.path=/ebi/production/interpro/binaries/64_bit_Linux/HMMER3.1b1/hmmscan
#binary.hmmer3.hmmsearch.path=/ebi/production/interpro/binaries/64_bit_Linux/HMMER3.1b1/hmmsearch
#binary.hmmer2.hmmsearch.path=/ebi/production/interpro/binaries/64_bit_Linux/HMMER2.3.2/hmmsearch
#binary.hmmer2.hmmpfam.path=/ebi/production/interpro/binaries/64_bit_Linux/HMMER2.3.2/hmmpfam
#binary.fingerprintscan.path=/ebi/production/interpro/binaries/64_bit_Linux/fingerPRINTScan
#binary.coils.path=/ebi/production/interpro/binaries/64_bit_Linux/ncoils/2.2.1/ncoils
#domainfinder3.path=/ebi/production/interpro/binaries/32_bit_Linux/DomainFinder3
#binary.prodom.2006.1.prodomblast3i.pl.path=bin/prodom/2006.1/ProDomBlast3i.pl
## Note: Correct prosite binary distribution for your platform can be downloaded: ftp://ftp.expasy.org/databases/prosite/ps_scan/
#binary.prosite.psscan.pl.path=/ebi/production/interpro/binaries/scripts/ps_scan.pl
#binary.prosite.pfscan.path=/ebi/production/interpro/binaries/64_bit_Linux/pfscan
#binary.panther.path=bin/panther/7.0/pantherScore.pl
#binary.panther.perl.lib.dir=bin/panther/7.0/lib
#binary.superfamily.1.75.ass3.pl.path=bin/superfamily/1.75/ass3_single_threaded.pl
#binary.pirsf.pl.path=bin/pirsf/3.01/pirsf.pl
#binary.blastall.2.2.6.path=bin/blast/2.2.6/blastall
#binary.blast.2.2.19.path=bin/blast/2.2.19
#binary.getorf.path=bin/nucleotide/getorf
## Note: SignalP binary not distributed with InterProScan 5, please install separately e.g. in bin/signalp/4.0/signalp
#binary.signalp.4.0.path=
## Note: TMHMM binary not distributed with InterProScan 5, please install separately e.g. in bin/tmhmm/2.0c/decodeanhmm
#binary.tmhmm.path=
## Note: Phobius binary not distributed with InterProScan 5, please install separately e.g. in bin/phobius/1.01/phobius.pl
#binary.phobius.pl.path.1.01=
#
## Member database model / data file locations (alphabetically sorted)
#coils.new_coil.mat.path.2.2=data/coils/2.2/new_coil.mat # NB no longer needed
#gene3d.hmm.path.3.5.0=/ebi/production/interpro/data/members/gene3d/3.5.0/gene3d_classified.hmm
#gene3d.model2sf_map.path.3.5.0=/ebi/production/interpro/data/members/gene3d/3.5.0/model_to_family_map.csv
#hamap.profile.models.path.201311.27=/ebi/production/interpro/data/members/hamap/201311.27/hamap.prf
## It is IMPORTANT to set this temporary directory to a directory on LOCAL disk -
## network IO will slow the panther analysis down considerably.
#panther.temporary.file.directory=/tmp/
#panther.models.dir.9.0=/ebi/production/interpro/data/members/panther/9.0/
#Pfam-A.hmm.path.27.0=/ebi/production/interpro/data/members/pfam/27.0/Pfam-A.hmm
#Pfam-A.seed.path.27.0=/ebi/production/interpro/data/members/pfam/27.0/Pfam-A.seed
#Pfam-C.path.27.0=/ebi/production/interpro/data/members/pfam/27.0/Pfam-C
#pirsf.sfhmm.path.3.01=/ebi/production/interpro/data/members/pirsf/3.01/sf_hmm_all
#pirsf.dat.path.3.01=/ebi/production/interpro/data/members/pirsf/3.01/pirsf.dat
#prints.kdat.path.42.0=/ebi/production/interpro/data/members/prints/42.0/prints42_0.kdat
#prints.pval.path.42.0=/ebi/production/interpro/data/members/prints/42.0/prints.pval
#prints.hierarchy.path.42.0=/ebi/production/interpro/data/members/prints/42.0/FingerPRINTShierarchy.db
#prodom.ipr.path.2006.1=/ebi/production/interpro/data/members/prodom/2006.1/prodom.ipr
#prosite.models.path.20.105=/ebi/production/interpro/data/members/prosite/20.105/prosite.dat
#prosite.evaluator.models.path.20.105=/ebi/production/interpro/data/members/prosite/20.105/evaluator.dat
#signalp.4.0.perl.library.dir=bin/signalp/4.0/lib
## Note: Smart overlapping and threshold files not distributed with InterProScan 5, please install separately e.g. in data/smart/6.2
#smart.hmm.path.6.2=/ebi/production/interpro/data/members/smart/6.2/smart.HMMs
#smart.hmm.bin.path.6.2=/ebi/production/interpro/data/members/smart/6.2/smart.HMMs.bin
#smart.overlapping.path.6.2=/ebi/production/interpro/data/members/smart/6.2/overlapping
#smart.threshold.path.6.2=/ebi/production/interpro/data/members/smart/6.2/THRESHOLDS
#superfamily.hmm.path.3.0=/ebi/production/interpro/data/members/superfamily/1.75/hmmlib_1.75
#superfamily.self.hits.path.1.75=/ebi/production/interpro/data/members/superfamily/1.75/self_hits.tab
#superfamily.cla.path.1.75=/ebi/production/interpro/data/members/superfamily/1.75/dir.cla.scop.txt_1.75
#superfamily.model.tab.path.1.75=/ebi/production/interpro/data/members/superfamily/1.75/model.tab
#superfamily.pdbj95d.path.1.75=/ebi/production/interpro/data/members/superfamily/1.75/pdbj95d
#tigrfam.hmm.path.13.0=/ebi/production/interpro/data/members/tigrfam/13.0/TIGRFAMs_13.0_HMM.LIB
#tigrfam.hmm.path.15.0=/ebi/production/interpro/data/members/tigrfam/15.0/TIGRFAMs_15.0_HMM.LIB
## Note: TMHMM model files not distributed with InterProScan 5, please install separately e.g. in data/tmhmm/2.0/TMHMM2.0.model
#tmhmm.model.path=
#
###
### cpu options for parallel processing
###
#
##hmmer cpu options for the different jobs
#hmmer3.hmmsearch.cpu.switch.pfama=--cpu 1
#hmmer3.hmmsearch.cpu.switch.tigrfam=--cpu 1
#hmmer3.hmmsearch.cpu.switch.gene3d=--cpu 1
#hmmer3.hmmsearch.cpu.switch.superfamily=--cpu 1
#
#hmmer2.hmmpfam.cpu.switch.smart=--cpu 1
#hmmer2.hmmpfam.cpu.switch.pirsf=--cpu 1
#
##panther binary cpu options (for blastall and hmmsearch)
#panther.binary.cpu.switch=-c 4
#
##pirsf binary cpu options (for hmmscan)
#pirsf.pl.binary.cpu.switch=-cpu 4
#
#
## These values control the maximum number of proteins put through
## an analysis in one go - different algorithms have different optimum values.
## Note that if you suffer from out of memory errors, reducing these values
## will almost certainly help, but may reduce the speed of analysis.
#analysis.max.sequence.count.SIGNALP=100
#analysis.max.sequence.count.TMHMM=100
#analysis.max.sequence.count.PANTHER=100
#analysis.max.sequence.count.SMART=50
#analysis.max.sequence.count.TIGRFAM=100
#analysis.max.sequence.count.GENE3D=50
#analysis.max.sequence.count.PRINTS=100
#analysis.max.sequence.count.PROSITE_PROFILES=100
#analysis.max.sequence.count.PROSITE_PATTERNS=100
#analysis.max.sequence.count.PIRSF=50
#analysis.max.sequence.count.PRODOM=100
#analysis.max.sequence.count.SSF=50
#analysis.max.sequence.count.HAMAP=100
#analysis.max.sequence.count.PFAM=100
#analysis.max.sequence.count.COILS=100
#analysis.max.sequence.count.PHOBIUS=100
#
## This beta release runs on a single machine, so there is no need for a delay.  Leave this as 0.
##nfs.delay.milliseconds=0
#
## Set the number of embedded workers to the number of processors that you would like to employ
## on the machine you are using to run InterProScan.
##number.of.embedded.workers=4
#
## By default, if the sequence already has matches available from the EBI, this service will look them
## up for you.  Note - at present it will always return all the available matches, ignoring any -appl options
## set on the command line.
#precalculated.match.lookup.service.url=
#
##proxy set up
#precalculated.match.lookup.service.proxy.host=
#precalculated.match.lookup.service.proxy.port=3128
#
## Instructs I5 to completely clean up after itself - leave set to true.
##delete.temporary.directory.on.completion=true
#
## A list of TCP ports that should not be used for messaging. (Apart from this, only ports > 1024 and < 65535 will be used.)
#tcp.port.exclusion.list=3879,3878,3881,3882
#
## JOB: jobLoadNucleicAcidSequence
##getorf.minsize=50
#
## TRUE by default, which means all generated graphical output documents (only SVG at the moment) will be archived (using the Linux command tar).
## This simple switch allows you to switch the archive mode off (simply set it to FALSE).
#archiveSVGOutput=true
#
##number of embedded workers  for the master
#number.of.embedded.workers=5
#maxnumber.of.embedded.workers=5
#
##grid name
#grid.name=lsf
##grid.name=sge
#
##project name for this run  - use user.digest
#user.digest=i5ProdRun
#
##grid jobs limit
#grid.jobs.limit=1000
#
##time between each bjobs or qstat command to check the status of jobs on the cluster
#grid.check.interval.seconds=120
#
##allow master interproscan to run binaries ()
#master.can.run.binaries=true
#
##deal with unknown step states
#recover.unknown.step.state=false
#
##lsf/bsub commands
#grid.worker.submit.command=bsub -q production-rh6
#grid.worker.submit.high.memory.command=bsub -q production-rh6 -M 8192
#grid.master.submit.command=bsub -q production-rh6
#grid.master.submit.high.memory.command=bsub -q production-rh6 -M 8192
#
#
#worker.command=java -Xms32m -Xmx2048m -jar interproscan-5.jar
## This may be identical to the worker.command argument above, however you may choose to select
## a machine with a much larger available memory, for use when a StepExecution fails.
#worker.high.memory.command=java -Xms32m -Xmx2048m -jar interproscan-5.jar
#
##number of embedded workers for the workers
#worker.number.of.embedded.workers=2
#worker.maxnumber.of.embedded.workers=2
#
##number of connections for the master
#master.maxconsumers=64
#
##number of connections to the worker
#worker.maxconsumers=32
#
##throttled network?
#grid.throttle=true
#
#worker.maxunfinished.jobs=8
#
#max.tier.depth=1
#
#jvm.maximum.idle.time.seconds=300
#
## Spare worker creation
#spare.worker.start.delay.seconds=120
#spare.worker.repeat.interval.seconds=120
#
## If multiple hosts are sharing the same file system, a delay may be required to
## avoid stale NFS handles
#nfs.delay.milliseconds=5000
#
#
## EBI Specific settings
#ebi.uniparc.protein.load.size=100000
#ebi.uniparc.protein.load.start.delay.seconds=1
#ebi.uniparc.protein.load.repeat.interval.seconds=3600
#
#delete.temporary.directory.on.completion=true
#
## JOB: jobLoadNucleicAcidSequence
#getorf.minsize=50
#
##        600000 = 10 minutes
##        3600000 = 1 hour
##        7200000 = 2 hours
##        43200000 = 12 hours
#
##log4j file
#log4j.log.file=i5.log4j.out.txt
#log4j.jms.level=warn
#
## This allows I5 to run multiple instances of a serial group
## eg post-processing
## Otherwise this becomes a major bottleneck
#production.max.serial.group.executions=220
#
## Active MQ JMS broker temporary data directory
#jms.broker.temp.directory=activemq-data/localhost/tmp_storage
